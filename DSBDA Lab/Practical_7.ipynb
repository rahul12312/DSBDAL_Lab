{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce159bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c59cc39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c31d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenization serves as the foundational step in text analytics. This process entails breaking down a block of text into smaller, more manageable units, such as individual words or sentences. By doing so, tokenization enables more effective analysis and understanding of the text, as it transforms large, unstructured paragraphs into smaller, structured components that can be more easily processed and analyzed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89831d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization serves as the foundational step in text analytics.', 'This process entails breaking down a block of text into smaller, more manageable units, such as individual words or sentences.', 'By doing so, tokenization enables more effective analysis and understanding of the text, as it transforms large, unstructured paragraphs into smaller, structured components that can be more easily processed and analyzed.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text= sent_tokenize(text)\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b8fa733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'serves', 'as', 'the', 'foundational', 'step', 'in', 'text', 'analytics', '.', 'This', 'process', 'entails', 'breaking', 'down', 'a', 'block', 'of', 'text', 'into', 'smaller', ',', 'more', 'manageable', 'units', ',', 'such', 'as', 'individual', 'words', 'or', 'sentences', '.', 'By', 'doing', 'so', ',', 'tokenization', 'enables', 'more', 'effective', 'analysis', 'and', 'understanding', 'of', 'the', 'text', ',', 'as', 'it', 'transforms', 'large', ',', 'unstructured', 'paragraphs', 'into', 'smaller', ',', 'structured', 'components', 'that', 'can', 'be', 'more', 'easily', 'processed', 'and', 'analyzed', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70f68e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from', 'only', 'herself', 'him', \"should've\", \"wouldn't\", 'on', 'you', 'them', \"mustn't\", \"you're\", 'no', 'those', 'your', 'her', 'does', 'his', 'most', \"we'll\", 'when', \"haven't\", 't', 'while', \"it'd\", 'o', 'weren', 've', 'shan', 'll', 'was', \"we're\", 'during', 'me', 'themselves', 'couldn', 'myself', 'and', 'or', 'same', 'all', \"it's\", 'with', 'who', \"i've\", 'down', 'am', \"i'll\", \"she'll\", 'haven', 'she', 'himself', 'doing', 'again', 'these', 'shouldn', 'in', 'further', 'between', 'being', \"you'd\", \"she's\", 'yourselves', \"weren't\", 'than', 'once', 'out', 'over', 'do', 'own', 'because', 'd', 'the', 'any', 'after', \"i'm\", 'its', 'it', 'too', 'such', \"didn't\", 'will', 'yourself', 'wasn', \"he'll\", 'more', 'why', \"isn't\", 'isn', 'which', 'each', \"he'd\", 'itself', \"they'd\", 'until', 'below', 'other', 's', 'y', 'here', 'mustn', 'this', 'he', 'of', 'won', 'but', \"needn't\", 'had', 'their', 'before', 'above', \"it'll\", 'didn', \"shan't\", 'there', 'is', 'mightn', \"shouldn't\", 'has', \"that'll\", 'theirs', \"we'd\", \"they're\", 'if', 'where', 'off', \"they've\", 'as', 'at', 'we', 'doesn', 'just', \"couldn't\", \"doesn't\", 'hadn', 'i', \"mightn't\", 'ourselves', 'whom', 'our', 'some', \"we've\", \"he's\", \"don't\", \"you've\", 'few', 'for', 'now', 'ours', 'are', 'my', 'both', 'to', 'wouldn', \"hasn't\", 'having', 'were', 'ma', 'through', 'not', 're', 'have', 'under', \"won't\", 'very', 'a', 'yours', 'been', \"aren't\", 'that', 'did', 'can', 'into', 'aren', 'm', 'needn', 'up', 'be', \"hadn't\", 'don', 'should', 'so', 'about', \"they'll\", \"she'd\", 'by', \"you'll\", 'hers', 'nor', 'an', \"i'd\", 'they', \"wasn't\", 'what', 'how', 'ain', 'then', 'hasn', 'against'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0589686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
     ]
    }
   ],
   "source": [
    "text= \"How to remove stop words with NLTK library in Python?\"\n",
    "text= re.sub('[^a-zA-Z]', ' ',text)\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_text=[]\n",
    "for w in tokens:\n",
    " if w not in stop_words:\n",
    "  filtered_text.append(w)\n",
    "print(\"Tokenized Sentence:\",tokens)\n",
    "print(\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "302661ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\",\n",
    "\"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    " rootWord=ps.stem(w)\n",
    "print(rootWord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a63dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for 'studies' is 'study'\n",
      "Lemma for 'studying' is 'studying'\n",
      "Lemma for 'cries' is 'cry'\n",
      "Lemma for 'cry' is 'cry'\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\" \n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for '{}' is '{}'\".format(w, lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f5429b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('pink', 'NN')]\n",
      "[('sweater', 'NN')]\n",
      "[('fit', 'NN')]\n",
      "[('her', 'PRP$')]\n",
      "[('perfectly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"The pink sweater fit her perfectly\"\n",
    "words=word_tokenize(data)\n",
    "for word in words:\n",
    " print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40c0256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d21f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = 'Jupiter is the largest Planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e840a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "030993df",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "486574c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1405257043.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[63], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    numOfWordsB = dict.fromkeys(uniqueWords,0)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    " numOfWordsA[word] += 1\n",
    "    numOfWordsB = dict.fromkeys(uniqueWords,0)\n",
    "for word in bagOfWordsB:\n",
    "numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4c3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c298c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
